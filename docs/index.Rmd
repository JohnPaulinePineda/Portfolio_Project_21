---
title: 'R : Extracting Information Using Dimensionality Reduction Algorithms'
author: "John Pauline Pineda"
date: "December 28, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This document implements dimensionality reduction algorithms for extracting information using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>.    
|
| Dimensionality reduction is a form of unsupervised learning method aimed at reducing the number of features in a data set while retaining as much information as possible for purposes of lessening the complexity of the model, improving the performance of the learning algorithm or for formulating a more intuitive visualization of the data. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**stats**</mark>, <mark style="background-color: #CCECFF">**fastICA**</mark>, <mark style="background-color: #CCECFF">**NMF**</mark>, <mark style="background-color: #CCECFF">**umap**</mark> and <mark style="background-color: #CCECFF">**Rtsne**</mark> packages) attempt to transform and project the data onto a lower-dimensional space while preserving important information.
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**NCI6**</mark>  dataset from the  <mark style="background-color: #CCECFF">**ISLR**</mark> package was used for this illustrated example. Only a subset of observations representing major cancer types was used for the analysis.    
|
| Preliminary dataset assessment:
|
| **[A]** 40 rows (observations)
| 
| **[B]** 6831 columns (variables)
|      **[B.1]** 1/6831 label = <span style="color: #FF0000">labs</span> variable (factor)
|             **[B.1.1]** Category 1 = <span style="color: #FF0000">labs=NSCLC</span> 
|             **[B.1.2]** Category 2 = <span style="color: #FF0000">labs=RENAL</span> 
|             **[B.1.3]** Category 3 = <span style="color: #FF0000">labs=MELANOMA</span> 
|             **[B.1.4]** Category 4 = <span style="color: #FF0000">labs=BREAST</span> 
|             **[B.1.5]** Category 5 = <span style="color: #FF0000">labs=COLON</span> 
|      **[B.2]** 6830/6831 descriptors = 6830/6830 numeric
|     
| 
```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(ISLR)
library(pkgmaker)
library(NMF)
library(fastICA)
library(umap)
library(Rtsne)

##################################
# Loading source and
# formulating the train set
##################################
data(NCI60)
NCI60 <- as.data.frame(NCI60)

##################################
# Filtering in the data subset for analysis
# and setting appropriate variable types
##################################
NCI60 <- NCI60[NCI60$labs %in% c("BREAST",
                                 "RENAL",
                                 "MELANOMA",
                                 "NSCLC",
                                 "COLON"),]

NCI60$labs <- as.factor(NCI60$labs)

NCI60$labs <- factor(NCI60$labs,
                     levels=c("BREAST",
                                 "RENAL",
                                 "MELANOMA",
                                 "NSCLC",
                                 "COLON"))

##################################
# Performing a general exploration of the data set
##################################
dim(NCI60)
str(NCI60)
summary(NCI60)

##################################
# Formulating a data type assessment summary
##################################
PDA <- NCI60
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)
```
##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 610 variables with First.Second.Mode.Ratio>5.
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** High skewness observed for 14 variables with Skewness>3 or Skewness<(-3).
| 
| **[E]** Considering the unsupervised learning nature of the analysis, no data pre-processing was proceeded to address the data quality issues identified.
|
```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- NCI60

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all descriptors
##################################
DQA.Descriptors <- DQA

##################################
# Listing all numeric Descriptors
##################################
DQA.Descriptors.Numeric <- DQA.Descriptors[,sapply(DQA.Descriptors, is.numeric)]

if (length(names(DQA.Descriptors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Descriptors
##################################
DQA.Descriptors.Factor <- DQA.Descriptors[,sapply(DQA.Descriptors, is.factor)]

if (length(names(DQA.Descriptors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Factor),
  Column.Type=sapply(DQA.Descriptors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Numeric),
  Column.Type=sapply(DQA.Descriptors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Descriptors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Descriptors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Descriptors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Descriptors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Descriptors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Descriptors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))==0) {
  print("No factor descriptors noted.")
} else if (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric descriptors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric descriptors noted.")
}

```

##  1.3 Data Preprocessing

###  1.3.1 Centering and Scaling
|
| Centering and Scaling data assessment:
|
| **[A]** To maintain an objective comparison across the different descriptors, centering and scaling transformation was applied on the numeric variables. The <span style="color: #0000FF">center</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was implemented which subtracts the average value of a numeric variable to all the values. As a result of centering, the variables had zero mean values. In addition, the <span style="color: #0000FF">scale</span> method, also from the <mark style="background-color: #CCECFF">**caret**</mark> package, was applied which performs a center transformation with each value of the variable divided by its standard deviation. Scaling the data coerced the values to have a common standard deviation of one.
|
```{r section_1.3.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- NCI60

##################################
# Listing all descriptors
##################################
DPA.Descriptors <- DPA

##################################
# Listing all numeric descriptors
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors[,sapply(DPA.Descriptors, is.numeric)]

##################################
# Applying a center and scale data transformation
##################################
DPA.Descriptors.Numeric_CenteredScaled <- preProcess(DPA.Descriptors.Numeric, method = c("center","scale"))
DPA.Descriptors.Numeric_CenteredScaledTransformed <- predict(DPA.Descriptors.Numeric_CenteredScaled, DPA.Descriptors.Numeric)
row.names(DPA.Descriptors.Numeric_CenteredScaledTransformed) <- NULL

```

## 1.4 Data Exploration
|
| Exploratory data analysis:
|
| **[A]** Most descriptors demonstrated differential relationships across the different levels of the <span style="color: #FF0000">Cancer</span> variable. Although, as driven by the huge number, the best descriptors cannot be clearly established. 
|
```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
Cancer <- NCI60$labs
NCI60_Transformed <- cbind(DPA.Descriptors.Numeric_CenteredScaledTransformed, Cancer )

EDA <- as.data.frame(NCI60_Transformed)

##################################
# Creating a function to define the
# range of descriptors for plotting
##################################

featurePlotRange <- function(start,end){

  ##################################
  # Listing all Descriptors
  ##################################
  EDA.Descriptors <- EDA[,start:end]
  EDA.Descriptors.Numeric <- EDA.Descriptors[,sapply(EDA.Descriptors, is.numeric)]

  ##################################
  # Formulating the box plots
  ##################################
  featurePlotResult <- featurePlot(x = EDA.Descriptors.Numeric,
            y = EDA$Cancer,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|")

  return(featurePlotResult)

}

##################################
# Plotting the descriptors
##################################
for (i in 0:242){
  print(featurePlotRange(1+28*i,28*(i+1)))
}

featurePlotRange(1+28*243,6830)

```

## 1.5 Dimensionality Reduction

###  1.5.1 Principal Component Analysis (PCA)
|
| **[A]** The PCA algorithm was implemented only for descriptors using the <mark style="background-color: #CCECFF">**stats**</mark> package.  
|
| **[B]** Combining the reduced dimensions defined by the <span style="color: #FF0000">PC1</span> and <span style="color: #FF0000">PC2</span> components provided a sufficient differential representation of the cancer types. 
|
```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DR <- EDA

DR.Numeric <- DR[,sapply(DR, is.numeric)]
  
##################################
# Performing PCA
##################################
DR_PCA <- prcomp(DR.Numeric)

##################################
# Consolidating the PCA components
##################################
rownames(DR_PCA$x) <- NULL
DR_PCA$x <- as.data.frame(DR_PCA$x)
Cancer <- EDA$Cancer
(DR_PCA_FULL <- cbind(DR_PCA$x, Cancer))
DR_PCA_FULL$Algorithm <- rep("PCA",nrow(DR_PCA_FULL))

##################################
# Comparing the PCA components
##################################
splom(~DR_PCA_FULL[,c(1:5)],
      groups = DR_PCA_FULL$Cancer,
      pch = 16,
      cex = 2,
      alpha = 0.45,
      varnames = c("PC1","PC2","PC3","PC4","PC5"),
      auto.key = list(points = TRUE, space = "top"),
      main = "Reduced Dimensions : Principal Component Analysis (PCA)",
      xlab = "Top PCA Components" )

##################################
# Plotting the best PCA components
##################################
xyplot(PC1 ~ PC2,
       groups = DR_PCA_FULL$Cancer,
       data = DR_PCA_FULL,
       xlab = "PC2",
       ylab = "PC1",
       type = "p",
       pch = 16,
       cex = 3,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"),
       main = "PCA")

```

###  1.5.2 Singular Value Decomposition (SVD)
|
| **[A]** The SVD algorithm was implemented only for descriptors using the <mark style="background-color: #CCECFF">**stats**</mark> package.  
|
| **[B]** Combining the reduced dimensions defined by the <span style="color: #FF0000">SV1</span> and <span style="color: #FF0000">SV2</span> components provided a sufficient differential representation of the cancer types. 
|
```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DR <- EDA

DR.Numeric <- DR[,sapply(DR, is.numeric)]
  
##################################
# Performing SVD
##################################
DR_SVD <- svd(DR.Numeric)

##################################
# Consolidating the SVD components
##################################
rownames(DR_SVD$u) <- NULL
DR_SVD$u <- as.data.frame(DR_SVD$u)
Cancer <- EDA$Cancer
(DR_SVD_FULL <- cbind(DR_SVD$u, Cancer))
DR_SVD_FULL$Algorithm <- rep("SVD",nrow(DR_SVD_FULL))

##################################
# Comparing the SVD components
##################################
splom(~DR_SVD_FULL[,c(1:5)],
      groups = DR_SVD_FULL$Cancer,
      pch = 16,
      cex = 2,
      alpha = 0.45,
      varnames = c("SV1","SV2","SV3","SV4","SV5"),
      auto.key = list(points = TRUE, space = "top"),
      main = "Reduced Dimensions : Singular Value Decomposition (SVD)",
      xlab = "Top SVD Components" )

##################################
# Plotting the best SVD components
##################################
xyplot(V1 ~ V2,
       groups = DR_SVD_FULL$Cancer,
       data = DR_SVD_FULL,
       xlab = "SV2",
       ylab = "SV1",
       type = "p",
       pch = 16,
       cex = 3,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"),
       main = "SVD")

```

###  1.5.3 Independent Component Analysis (ICA)
|
| **[A]** The ICA algorithm was implemented only for descriptors using the <mark style="background-color: #CCECFF">**fastICA**</mark> package.  
|
| **[B]** Combining the reduced dimensions defined by the <span style="color: #FF0000">IC1</span> and <span style="color: #FF0000">IC2</span> components provided a sufficient differential representation of the cancer types. 
|
```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DR <- EDA

DR.Numeric <- DR[,sapply(DR, is.numeric)]

##################################
# Performing ICA
##################################
DR_ICA <- fastICA(DR.Numeric, n.comp=2)

##################################
# Consolidating the ICA components
##################################
rownames(DR_ICA$S) <- NULL
DR_ICA$S <- as.data.frame(DR_ICA$S)
Cancer <- EDA$Cancer
(DR_ICA_FULL <- cbind(DR_ICA$S, Cancer))
DR_ICA_FULL$Algorithm <- rep("ICA",nrow(DR_ICA_FULL))

##################################
# Plotting the best ICA components
##################################
xyplot(V1 ~ V2,
       groups = DR_ICA_FULL$Cancer,
       data = DR_ICA_FULL,
       xlab = "IC2",
       ylab = "IC1",
       type = "p",
       pch = 16,
       cex = 3,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"),
       main = "ICA")

```

###  1.5.4 Non-Negative Matrix Factorization (NMF)
|
| **[A]** The NMF algorithm was implemented only for descriptors using the <mark style="background-color: #CCECFF">**NMF**</mark> package.  
|
| **[B]** Combining the reduced dimensions defined by the <span style="color: #FF0000">NMF1</span> and <span style="color: #FF0000">NMF2</span> components provided a sufficient differential representation of the cancer types. 
|
```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DR <- EDA

DR.Numeric <- DR[,sapply(DR, is.numeric)]

##################################
# Conducting max-min normalization
# to transform all descriptors
# to positive numeric values
##################################
Min_Max_Normalization <- function(x, na.rm = TRUE) {
    return((x- min(x)) /(max(x)-min(x)))
}

DR.Numeric.MMNScaled <- apply(DR.Numeric, 2, Min_Max_Normalization)

##################################
# Performing NMF
##################################
DR_NMF <- nmf(DR.Numeric.MMNScaled, rank=2)

##################################
# Consolidating the NMF components
##################################
DR_NMF_fitW <- as.data.frame(DR_NMF@fit@W)
rownames(DR_NMF_fitW) <- NULL
Cancer <- EDA$Cancer
(DR_NMF_FULL <- cbind(DR_NMF_fitW, Cancer))
DR_NMF_FULL$Algorithm <- rep("NMF",nrow(DR_NMF_FULL))

##################################
# Plotting the best NMF components
##################################
xyplot(V1 ~ V2,
       groups = DR_NMF_FULL$Cancer,
       data = DR_NMF_FULL,
       xlab = "NMF2",
       ylab = "NMF1",
       type = "p",
       pch = 16,
       cex = 3,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"),
       main = "NMF")
```

###  1.5.5 t-Distributed Stochastic Neighbor Embedding (tSNE)
|
| **[A]** The tSNE algorithm was implemented only for descriptors using the <mark style="background-color: #CCECFF">**Rtsne**</mark> package.  
|
| **[B]** Combining the reduced dimensions defined by the <span style="color: #FF0000">tSNE1</span> and <span style="color: #FF0000">tSNE2</span> components provided a sufficient differential representation of the cancer types. 
|
```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DR <- EDA

DR.Numeric <- DR[,sapply(DR, is.numeric)]

##################################
# Performing tSNE
##################################
set.seed(12345678)
DR_tSNE <- Rtsne(DR.Numeric, perplexity=5)

##################################
# Consolidating the tSNE components
##################################
rownames(DR_tSNE$Y) <- NULL
DR_tSNE$Y <- as.data.frame(DR_tSNE$Y)
Cancer <- EDA$Cancer
(DR_tSNE_FULL <- cbind(DR_tSNE$Y, Cancer))
DR_tSNE_FULL$Algorithm <- rep("tSNE",nrow(DR_tSNE_FULL))

##################################
# Plotting the best tSNE components
##################################
xyplot(V1 ~ V2,
       groups = DR_tSNE_FULL$Cancer,
       data = DR_tSNE_FULL,
       xlab = "tSNE2",
       ylab = "tSNE1",
       type = "p",
       pch = 16,
       cex = 3,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"),
       main = "tSNE")

```

###  1.5.6 Uniform Manifold Approximation and Projection (UMAP)
|
| **[A]** The UMAP algorithm was implemented only for descriptors using the <mark style="background-color: #CCECFF">**umap**</mark> package.  
|
| **[B]** Combining the reduced dimensions defined by the <span style="color: #FF0000">UMAP1</span> and <span style="color: #FF0000">UMAP2</span> components provided a sufficient differential representation of the cancer types. 
|
```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DR <- EDA

DR.Numeric <- DR[,sapply(DR, is.numeric)]

##################################
# Performing UMAP
##################################
set.seed(12345678)
DR_UMAP <- umap(DR.Numeric)

##################################
# Consolidating the UMAP components
##################################
rownames(DR_UMAP$layout) <- NULL
DR_UMAP$layout <- as.data.frame(DR_UMAP$layout)
Cancer <- EDA$Cancer
(DR_UMAP_FULL <- cbind(DR_UMAP$layout, Cancer))
DR_UMAP_FULL$Algorithm <- rep("UMAP",nrow(DR_UMAP_FULL))

##################################
# Plotting the best UMAP components
##################################
xyplot(V1 ~ V2,
       groups = DR_UMAP_FULL$Cancer,
       data = DR_UMAP_FULL,
       xlab = "UMAP2",
       ylab = "UMAP1",
       type = "p",
       pch = 16,
       cex = 3,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"),
       main = "UMAP")

```

##  1.6 Algorithm Comparison Summary
|
| Algorithm performance comparison:
|
| **[A]** The dimensionality reduction algorithms applied only to the descriptors were able to effectively capture the latent characteristics of the cancer types. 
|      **[A.1]** PCA: Principal Component Analysis (<mark style="background-color: #CCECFF">**stats**</mark> package)
|      **[A.2]** SVD: Singular Value Decomposition (<mark style="background-color: #CCECFF">**stats**</mark> package)
|      **[A.3]** ICA: Independent Component Analysis (<mark style="background-color: #CCECFF">**fastICA**</mark> package)
|      **[A.4]** NMF: Non-Negative Matrix Factorization (<mark style="background-color: #CCECFF">**NMF**</mark> package)
|      **[A.5]** tSNE: t-Distributed Stochastic Neighbor Embedding (<mark style="background-color: #CCECFF">**Rtsne**</mark> package)
|      **[A.6]** UMAP: Uniform Manifold Approximation and Projection (<mark style="background-color: #CCECFF">**umap**</mark> package)
|
```{r section_1.6, warning=FALSE, message=FALSE}
##################################
# Re-plotting the best PCA components
##################################
PCA_Plot <- xyplot(PC1 ~ PC2 | Algorithm,
       groups = DR_PCA_FULL$Cancer,
       data = DR_PCA_FULL,
       xlab = "PC2",
       ylab = "PC1",
       type = "p",
       pch = 16,
       cex = 2,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"))

##################################
# Re-plotting the best SVD components
##################################
SVD_Plot <- xyplot(V1 ~ V2 | Algorithm,
       groups = DR_SVD_FULL$Cancer,
       data = DR_SVD_FULL,
       xlab = "SV2",
       ylab = "SV1",
       type = "p",
       pch = 16,
       cex = 2,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"))

##################################
# Re-plotting the best ICA components
##################################
ICA_Plot <- xyplot(V1 ~ V2 | Algorithm,
       groups = DR_ICA_FULL$Cancer,
       data = DR_ICA_FULL,
       xlab = "IC2",
       ylab = "IC1",
       type = "p",
       pch = 16,
       cex = 2,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"))

##################################
# Re-plotting the best NMF components
##################################
NMF_Plot <- xyplot(V1 ~ V2 | Algorithm,
       groups = DR_NMF_FULL$Cancer,
       data = DR_NMF_FULL,
       xlab = "NMF2",
       ylab = "NMF1",
       type = "p",
       pch = 16,
       cex = 2,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"))

##################################
# Re-plotting the best tSNE components
##################################
tSNE_Plot <- xyplot(V1 ~ V2 | Algorithm,
       groups = DR_tSNE_FULL$Cancer,
       data = DR_tSNE_FULL,
       xlab = "tSNE2",
       ylab = "tSNE1",
       type = "p",
       pch = 16,
       cex = 2,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"))

##################################
# Re-plotting the best UMAP components
##################################
UMAP_Plot <- xyplot(V1 ~ V2 | Algorithm,
       groups = DR_UMAP_FULL$Cancer,
       data = DR_UMAP_FULL,
       xlab = "UMAP2",
       ylab = "UMAP1",
       type = "p",
       pch = 16,
       cex = 2,
       alpha = 0.45,
       auto.key = list(points = TRUE, space = "top"))

################################################################################
# Consolidating all algorithm performance results
################################################################################
grid.arrange(PCA_Plot, 
             SVD_Plot, 
             ICA_Plot,
             NMF_Plot,
             tSNE_Plot,
             UMAP_Plot,
             ncol = 3)

```

# **2. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [ISLR](https://cran.r-project.org/web/packages/ISLR/ISLR.pdf) by Trevor Hastie
| **[R Package]** [pkgmaker](https://cran.r-project.org/web/packages/pkgmaker/index.html) by R Core Team
| **[R Package]** [NMF](https://cran.r-project.org/web/packages/NMF/NMF.pdf) by Renaud Gaujoux and Cathal Seoighe
| **[R Package]** [fastICA](https://cran.r-project.org/web/packages/fastICA/fastICA.pdf) by Brian Ripley
| **[R Package]** [umap](https://cran.r-project.org/web/packages/umap/umap.pdf) by R Tomasz Konopka
| **[R Package]** [Rtsne](https://cran.r-project.org/web/packages/Rtsne/Rtsne.pdf) by Jesse Krijthe
| **[Article]** [6 Dimensionality Reduction Techniques in R (with Examples)](https://cmdlinetips.com/2022/07/dimensionality-reduction-techniques-in-r/) by CMDLineTips Team
| **[Article]** [6 Dimensionality Reduction Algorithms With Python](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction](https://www.geeksforgeeks.org/dimensionality-reduction/) by Geeks For Geeks
| **[Article]** [Principal Component Analysis for Dimensionality Reduction in Python](https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/) by Jason Brownlee
| **[Article]** [Principal Component Analysis Explained Simply](https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/) by Linh Ngo
| **[Article]** [A Step-by-Step Explanation of Principal Component Analysis (PCA)](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) by Zakaria Jaadi
| **[Article]** [What Is Principal Component Analysis (PCA) and How It Is Used?](https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186) by Sartorius Team
| **[Article]** [Principal Components Analysis](https://online.stat.psu.edu/stat508/book/export/html/639) by by Penn State Eberly College of Science
| **[Article]** [Principal Component Analysis – How PCA Algorithms Works, The Concept, Math and Implementation](https://www.machinelearningplus.com/machine-learning/principal-components-analysis-pca-better-explained/) by Selva Prabhakaran
| **[Article]** [Singular Value Decomposition for Dimensionality Reduction in Python](https://machinelearningmastery.com/singular-value-decomposition-for-dimensionality-reduction-in-python/) by Jason Brownlee
| **[Article]** [Singular Value Decomposition (SVD)](https://www.geeksforgeeks.org/singular-value-decomposition-svd/) by Geeks For Geeks
| **[Article]** [Singular Value Decomposition as Simply as Possible](https://gregorygundersen.com/blog/2018/12/10/svd/) by Gregory Gundersen
| **[Article]** [Singular Value Decomposition](https://datascience.eu/machine-learning/singular-value-decomposition/) by Data Science Team
| **[Article]** [Singular Value Decomposition (SVD)](https://www.machinelearningmindset.com/singular-value-decomposition-svd/) by Maachine Learning Mindset Team
| **[Article]** [Singular Value Decomposition](https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recommender/singular_value_decomposition) by Calvin Feng
| **[Article]** [ML: Independent Component Analysis](https://www.geeksforgeeks.org/ml-independent-component-analysis/) by Geeks For Geeks Team
| **[Article]** [A Tutorial on Independent Component Analysis](https://arxiv.org/pdf/1404.2986v1.pdf) by Jonathon Shlens
| **[Article]** [Introduction to Independent Component Analysis in Machine Learning](https://www.analyticssteps.com/blogs/introduction-independent-component-analysis-machine-learning) by Soumyaa Rawat
| **[Article]** [Non-Negative Matrix Factorization](https://www.geeksforgeeks.org/non-negative-matrix-factorization/) by Geeks For Geeks Team
| **[Article]** [Non-Negative Matrix Factorization For Dimensionality Reduction](https://predictivehacks.com/non-negative-matrix-factorization-for-dimensionality-reduction/) by George Pipis
| **[Article]** [Non-Negative Matrix Factorization](https://danhdtruong.com/Non-negative-Matrix-Factorization/) by Danh Truong
| **[Article]** [The Why and How of Non-Negative Matrix Factorization](https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/) by Adrian Colyer
| **[Article]** [t-SNE: T-Distributed Stochastic Neighbor Embedding Explained](https://learnopencv.com/t-sne-t-distributed-stochastic-neighbor-embedding-explained/) by Prakash Chandra
| **[Article]** [What is tSNE?](https://sonraianalytics.com/what-is-tsne/) by Matt Lee
| **[Article]** [ML: T-distributed Stochastic Neighbor Embedding (t-SNE) Algorithm](https://www.geeksforgeeks.org/ml-t-distributed-stochastic-neighbor-embedding-t-sne-algorithm/) by Geeks For Geeks Team
| **[Article]** [An illustrated introduction to the t-SNE algorithm](https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/) by Cyrille Rossant
| **[Article]** [Understanding UMAP](https://pair-code.github.io/understanding-umap/) by Andy Coenen and Adam Pearce
| **[Article]** [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://www.jkobject.com/blog/umap-explanation/) by Jeremie Kalfon
| **[Publication]** [Analysis of a Complex of Statistical Variables into Principal Components](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fh0071325) by Harold Hotelling (Journal of Educational Psychology)
| **[Publication]** [The Approximation of One Matrix by Another of Lower Rank](https://link.springer.com/article/10.1007/BF02288367) by Carl Eckart and Gale Young 
| **[Publication]** [Title](https://www.jkobject.com/blog/umap-explanation/) by Author
| **[Publication]** [Title](https://www.jkobject.com/blog/umap-explanation/) by Author
| **[Publication]** [Title](https://www.jkobject.com/blog/umap-explanation/) by Author
| **[Publication]** [Title](https://www.jkobject.com/blog/umap-explanation/) by Author
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|